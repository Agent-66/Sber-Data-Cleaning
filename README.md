
![](./images/data_cleaning.png)
# <center> Очистка данных на Python </center>

## Оглавление
1. [Описание проекта](https://github.com/Agent-66/Sber-Data-Cleaning/blob/master/README.md#Описание-проекта)
2. [Описание данных](https://github.com/Agent-66/Sber-Data-Cleaning/blob/master/README.md#Описание-данных)
3. [Зависимости](https://github.com/Agent-66/Sber-Data-Cleaning/blob/master/README.md#Зависимости)
4. [Установка проекта](https://github.com/Agent-66/Sber-Data-Cleaning/blob/master/README.md#Установка-проекта)
5. [Использование проекта](https://github.com/Agent-66/Sber-Data-Cleaning/blob/master/README.md#Использование-проекта)
6. [Авторы](https://github.com/Agent-66/Sber-Data-Cleaning/blob/master/README.md#Авторы)
7. [Выводы](https://github.com/Agent-66/Sber-Data-Cleaning/blob/master/README.md#Выводы)

## Описание проекта

> **Очистка данных (data cleaning)** – это процесс обнаружения и удаления (или исправления) поврежденных, ложных или неинформативных записей таблицы или целой базы данных. Процесс состоит из двух этапов: поиск и ликвидация (или редактирование).

Основные этапы очистки данных:
* Работа с пропущенными значениями.
* Очистка данных от пропусков.
* Удаление признаков и записей, которые не несут полезной информации.

**Цель очистки данных** — избавиться от «мусора», который может помешать моделированию или исказить его результаты. Во многих задачах очистка данных — это самая главная часть этапа подготовки данных к построению модели, которая нередко занимает большую часть времени работы над задачей.

Рассмотрим пример того, как «мусор» может влиять на результат. На графиках ниже представлены две диаграммы рассеяния и две одинаковые линейные модели (прямые, которые пытаются повторить данные). На левом графике модель построена на «грязных» данных, содержащих аномалии, а на правом модель обучена на очищенных данных. 

![](./images/example_outliers.png)

**Данный проект** направлен на демонстрацию применения различных методов очистки данных на каждом из ее этапов на примере датасета о квартирах в Москве от Сбербанка.

**О структуре проекта:**
* [data](./data) - папка с исходными табличными данными;
* [images](./images) - папка с изображениями, необходимыми для проекта;
* [outliers_lib](./outliers_lib) - папка со вспомогательными модулями для обработки выбросов; 
* [data_cleaning_example.ipynb](./data_cleaning_example.ipynb) - jupyter-ноутбук, содержащий основной код проекта, в котором демонстрируются методы и подходы решения задач очистки данных.


## Описание данных

В этом проекте используются данные с соревнования [Sberbank Russian Housing Market](https://www.kaggle.com/c/sberbank-russian-housing-market/data) от Сбера (бывшый Сбербанк).

Требования Сбера состояли в построении модели, которая бы прогнозировала цены на жильё в Москве, опираясь на параметры самого жилья, а также состояние экономики и финансового сектора в стране.

Исходный датасет представляет собой набор данных из таблицы с информацией о параметрах жилья в Москве и Московской области, а также таблицы, в которой содержатся 292 признака о состоянии экономики России на момент продажи недвижимости. 

Для упрощения демонстрации техники очистки данных мы будем отрабатывать на урезанном датасете. Он содержит информацию о 61 признаке, описывающих жилье. Файл с данными можно найти [здесь](./data/sber_data.csv).

## Зависимости

Python (3.10.6):
* [numpy (1.24.1)](https://numpy.org)
* [pandas (1.5.2)](https://pandas.pydata.org)
* [matplotlib (3.6.2)](https://matplotlib.org)
* [seaborn (0.12.2)](https://seaborn.pydata.org)

## Установка проекта

```
git clone https://github.com/Agent-66/Sber-Data-Cleaning
```

## Использование проекта

Вся информация о работе представлена в jupyter-ноутбуке data_cleaning_example.ipynb.

## Авторы

* [Andrey Razin (Agent-66)](https://github.com/Agent-66)

## Выводы

В процессе работы над проектом была произведена предобработка данных в датасете об объектах недвижимости в Москве и очистка данных от пропусков, выбросов и дубликатов.

1. Поиск и обработка пропущенных значений. Рассмотрели различные методы работы с пропусками, их преимущества и недостатки. В итоге подобрали оптимальную комбинацию методов для данного датасета: удалили столбец, в котором было более 30% пропусков, удалили записи, в которых было более двух пропусков одновременно, оставшиеся пропуски в данных заполнили константами.
2. Работа с выбросами. Проанализировали данные ручным методом (поиск и удаление значений, противоречащих здравому смыслу). Для поиска неявных аномалий воспользовались методом межквантильного размаха и методом z-отклонений (найденые выбросы были также проанализированы и удалены из таблицы).
3. Поиск и ликвидация дубликатов. Из таблицы были удалены все полные дубликаты записей.

Таким образом, в результате получили очищенный датасет без пропусков, готовый для построения модели согласно поставленной задаче от Сбера.



